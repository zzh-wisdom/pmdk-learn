# 源码阅读

## 关于granularity

传入cfg中granularity只是限制最大的granularity，具体是多少，pmdk会根据环境来选择最小的粒度。

```cpp
enum pmem2_granularity available_min_granularity =
		src->type == PMEM2_SOURCE_ANON ? PMEM2_GRANULARITY_BYTE :
		get_min_granularity(eADR, map_sync, cfg->sharing);

	if (available_min_granularity > cfg->requested_max_granularity) {
		const char *err = granularity_err_msg
			[cfg->requested_max_granularity]
			[available_min_granularity];
		if (strcmp(err, GRAN_IMPOSSIBLE) == 0)
			FATAL(
				"unhandled granularity error: available_min_granularity: %d" \
				"requested_max_granularity: %d",
				available_min_granularity,
				cfg->requested_max_granularity);
		ERR("%s", err);
		ret = PMEM2_E_GRANULARITY_NOT_SUPPORTED;
		goto err_undo_mapping;
	}

// ...

map->effective_granularity = available_min_granularity;
```

## 关于flush drain函数

```cpp
void
pmem2_set_flush_fns(struct pmem2_map *map)
{
	switch (map->effective_granularity) {
		case PMEM2_GRANULARITY_PAGE:
			map->persist_fn = pmem2_persist_pages;
			map->flush_fn = pmem2_persist_pages;
			map->drain_fn = pmem2_drain_nop;
			map->deep_flush_fn = pmem2_deep_flush_page;
			break;
		case PMEM2_GRANULARITY_CACHE_LINE:
			map->persist_fn = pmem2_persist_cpu_cache;
			map->flush_fn = pmem2_flush_cpu_cache;
			map->drain_fn = pmem2_drain;
			map->deep_flush_fn = pmem2_deep_flush_cache;
			break;
		case PMEM2_GRANULARITY_BYTE:
			map->persist_fn = pmem2_persist_noflush;
			map->flush_fn = pmem2_flush_nop;
			map->drain_fn = pmem2_drain;
			map->deep_flush_fn = pmem2_deep_flush_byte;
			break;
		default:
			abort();
	}
}
```

除了PMEM2_GRANULARITY_PAGE，其他都有fence，除了PMEM2_GRANULARITY_BYTE，其他都有flush

drain函数使用`_mm_sfence()`

```cpp
/*
 * pmem_get_cpuinfo -- configure libpmem based on CPUID
 */
static void
pmem_cpuinfo_to_funcs(struct pmem2_arch_info *info, enum memcpy_impl *impl)
{
	LOG(3, NULL);

	if (is_cpu_clflush_present()) {
		LOG(3, "clflush supported");

		info->flush = flush_clflush;
		info->flush_has_builtin_fence = 1;
		info->fence = memory_barrier;
	}

	if (is_cpu_clflushopt_present()) {
		LOG(3, "clflushopt supported");

		char *e = os_getenv("PMEM_NO_CLFLUSHOPT");
		if (e && strcmp(e, "1") == 0) {
			LOG(3, "PMEM_NO_CLFLUSHOPT forced no clflushopt");
		} else {
			info->flush = flush_clflushopt;
			info->flush_has_builtin_fence = 0;
			info->fence = memory_barrier;
		}
	}

	if (is_cpu_clwb_present()) {
		LOG(3, "clwb supported");

		char *e = os_getenv("PMEM_NO_CLWB");
		if (e && strcmp(e, "1") == 0) {
			LOG(3, "PMEM_NO_CLWB forced no clwb");
		} else {
			info->flush = flush_clwb;
			info->flush_has_builtin_fence = 0;
			info->fence = memory_barrier;
		}
	}

	/*
	 * XXX Disable this work around for Intel CPUs with optimized
	 * WC eviction.
	 */
	int wc_workaround = is_cpu_genuine_intel();

	char *ptr = os_getenv("PMEM_WC_WORKAROUND");
	if (ptr) {
		if (strcmp(ptr, "1") == 0) {
			LOG(3, "WC workaround forced to 1");
			wc_workaround = 1;
		} else if (strcmp(ptr, "0") == 0) {
			LOG(3, "WC workaround forced to 0");
			wc_workaround = 0;
		} else {
			LOG(3, "incorrect value of PMEM_WC_WORKAROUND (%s)",
				ptr);
		}
	}
	LOG(3, "WC workaround = %d", wc_workaround);

	ptr = os_getenv("PMEM_NO_MOVNT");
	if (ptr && strcmp(ptr, "1") == 0) {
		LOG(3, "PMEM_NO_MOVNT forced no movnt");
	} else {
		/*
		 * pmem_set_mem_funcs is not used at all when all available
		 * operations are disabled
		 */
		SUPPRESS_UNUSED(pmem_set_mem_funcs);

		use_sse2_memcpy_memset(info, impl, wc_workaround);

		if (is_cpu_avx_present())
			use_avx_memcpy_memset(info, impl, wc_workaround);

		if (is_cpu_avx512f_present())
			use_avx512f_memcpy_memset(info, impl);

		if (is_cpu_movdir64b_present())
			use_movdir64b_memcpy_memset(info, impl);
	}
}
```

// 可以看到，pmdk优先使用clwb进行flush

```cpp
#define force_inline __attribute__((always_inline)) inline

static force_inline void
pmem_clflushopt(const void *addr)
{
	asm volatile(".byte 0x66; clflush %0" : "+m" \
		(*(volatile char *)(addr)));
}
static force_inline void
pmem_clwb(const void *addr)
{
	asm volatile(".byte 0x66; xsaveopt %0" : "+m" \
		(*(volatile char *)(addr)));
}

static force_inline void
flush_clwb_nolog(const void *addr, size_t len)
{
	uintptr_t uptr;

	/*
	 * Loop through cache-line-size (typically 64B) aligned chunks
	 * covering the given range.
	 */
	for (uptr = (uintptr_t)addr & ~(FLUSH_ALIGN - 1);
		uptr < (uintptr_t)addr + len; uptr += FLUSH_ALIGN) {
		pmem_clwb((char *)uptr);
	}
}
```

也是按照cacheline进行flush。

## 关于memory类函数

```cpp
int
pmem2_map_new(struct pmem2_map **map_ptr, const struct pmem2_config *cfg,
		const struct pmem2_source *src)

void
pmem2_set_mem_fns(struct pmem2_map *map)
{
	switch (map->effective_granularity) {
		case PMEM2_GRANULARITY_PAGE:
			map->memmove_fn = pmem2_memmove_nonpmem;
			map->memcpy_fn = pmem2_memmove_nonpmem;
			map->memset_fn = pmem2_memset_nonpmem;
			break;
		case PMEM2_GRANULARITY_CACHE_LINE:
			map->memmove_fn = pmem2_memmove;
			map->memcpy_fn = pmem2_memmove;
			map->memset_fn = pmem2_memset;
			break;
		case PMEM2_GRANULARITY_BYTE:
			map->memmove_fn = pmem2_memmove_eadr;
			map->memcpy_fn = pmem2_memmove_eadr;
			map->memset_fn = pmem2_memset_eadr;
			break;
		default:
			abort();
	}

}

static void
pmem_set_mem_funcs(struct pmem2_arch_info *info)
{
	info->memmove_nodrain = pmem_memmove_nodrain;
	info->memmove_nodrain_eadr = pmem_memmove_nodrain_eadr;
	info->memset_nodrain = pmem_memset_nodrain;
	info->memset_nodrain_eadr = pmem_memset_nodrain_eadr;
}

use_sse2_memcpy_memset

static void
pmem_cpuinfo_to_funcs(struct pmem2_arch_info *info, enum memcpy_impl *impl)

/*
 * use_movdir64b_memcpy_memset -- (internal) movdir64b detected, use it if
 *                                           possible
 */
static void
use_movdir64b_memcpy_memset(struct pmem2_arch_info *info,
		enum memcpy_impl *impl)
{
#if MOVDIR64B_AVAILABLE
	LOG(3, "movdir64b supported");

	char *e = os_getenv("PMEM_MOVDIR64B");
	if (e != NULL && strcmp(e, "0") == 0) {
		LOG(3, "PMEM_MOVDIR64B set to 0");
		return;
	}

	LOG(3, "PMEM_MOVDIR64B enabled");
	*impl = MEMCPY_MOVDIR64B;

	pmem_set_mem_funcs(info);

	info->memmove_funcs.nt.noflush = memmove_movnt_movdir64b_noflush;
	info->memmove_funcs.nt.empty = memmove_movnt_movdir64b_empty;
	info->memset_funcs.nt.noflush = memset_movnt_movdir64b_noflush;
	info->memset_funcs.nt.empty = memset_movnt_movdir64b_empty;

	if (info->flush == flush_clflush) {
		info->memmove_funcs.nt.flush = memmove_movnt_movdir64b_clflush;
		info->memset_funcs.nt.flush = memset_movnt_movdir64b_clflush;
	} else if (info->flush == flush_clflushopt) {
		info->memmove_funcs.nt.flush =
				memmove_movnt_movdir64b_clflushopt;
		info->memset_funcs.nt.flush = memset_movnt_movdir64b_clflushopt;
	} else if (info->flush == flush_clwb) {
		info->memmove_funcs.nt.flush = memmove_movnt_movdir64b_clwb;
		info->memset_funcs.nt.flush = memset_movnt_movdir64b_clwb;
	} else {
		ASSERT(0);
	}
#else
	SUPPRESS_UNUSED(info, impl);
	LOG(3, "movdir64b supported, but disabled at build time");
#endif
}

memset_movnt_avx512f_clwb(char *dest, int c, size_t len)
memset_movnt_movdir64b_clwb(char *dest, int c, size_t len)

// avx512f
static __inline__ void __DEFAULT_FN_ATTRS512
_mm512_stream_si512 (void * __P, __m512i __A)
{
  typedef __v8di __v8di_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v8di_aligned)__A, (__v8di_aligned*)__P);
}

static __inline void __DEFAULT_FN_ATTRS
_mm256_stream_si256(__m256i *__a, __m256i __b)
{
  typedef __v4di __v4di_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v4di_aligned)__b, (__v4di_aligned*)__a);
}

// movdir64b
static __inline__ void
__attribute__((__always_inline__, __nodebug__,  __target__("movdir64b")))
_movdir64b (void *__dst __attribute__((align_value(64))), const void *__src)
{
  __builtin_ia32_movdir64b(__dst, __src);
}
```

memcpy和memmove的底层实现是一样的。
